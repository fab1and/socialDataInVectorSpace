############################
# pre processing strategy###
############################

# transform emoijys to text that can be analyzed by nlp engine
# drop links
# stemming
# lammatization
# use preprocessing of nlp models
import pandas as pd
import numpy as np
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import SpaceTokenizer
from nltk.stem import WordNetLemmatizer
#from Src.prompt import prompt
#import imblearn
#from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN

def removeQuotes(data, cols):

    from tqdm import tqdm
    import warnings
    warnings.filterwarnings("ignore")

    col_names = data.columns.values

    for col in range(len(col_names)):

        col_names[col] = col_names[col].replace('"', '')

    data.columns = col_names

    for col in cols:

        print(f'cleaning {col}')

        for row in tqdm(range(data.shape[0])):

            data[col][row] = str(data[col][row]).replace('"', '')

    return data

def cleanAppend(company,type,portions):

    import pandas as pd

    final_dat = pd.read_csv(f'C:/Users/Fabia/pythoninput/{type}_{company}_partly_instagram.csv').drop('Unnamed: 0', axis = 1)
    max_post_id = final_dat['post_id'].max()
    latest_dat = final_dat

    for data in range(2,portions + 1):

        try:

            dat_ = pd.read_csv(f'C:/Users/Fabia/pythoninput/{type}_{company}_partly{data}_instagram.csv').drop('Unnamed: 0', axis = 1)
            dat_['post_id'] = dat_['post_id'] + max_post_id
            latest_dat = dat_
            max_post_id = dat_['post_id'].max()

            final_dat = final_dat.append(dat_)
        except:
            pass
    print(f'{type} data was generated by {portions} appended dataframes for {company}')

    return final_dat.reset_index(drop = True)

def calcDateTime(data, col, extract_years = True):
    """
    :param data: dataframe
    :param column: name of the column in which week information is storaged
    :return: changes the time column into date
    """
    from datetime import datetime,timedelta
    from tqdm import tqdm
    import warnings
    warnings.filterwarnings("ignore")

    data[col] = data[col].fillna(0)

    data['date'] = 0

    print('calculating dates')

    data.reset_index(drop = True, inplace = True)

    for i in tqdm(range(data.shape[0])):
        try:
            if 'Wo' in data[col][i]:

                data['date'][i] = (datetime.today().date() - timedelta(weeks = int(data[col][i].split(' ')[0])))

            elif 'Tage' in data[col][i]:

                data['date'][i] = (datetime.today().date() - timedelta(days = int(data[col][i].split(' ')[0])))

            elif 'Std' in data[col][i]:
                data['date'][i] = (datetime.today().date() - timedelta(hours = int(data[col][i].split(' ')[0])))


            else:
                data['date'][i] = data[col][i]


        except:
            data['date'][i] = data[col][i]

    if extract_years == True:

        print('getting year of dates')

        data['year'] = 0

        for i in tqdm(range(data.shape[0])):
            try:
                try:
                    data['year'][i] = data['date'][i].year
                except:
                    data['year'][i] = data.loc[i][' time'][6:10]
            except:
                pass
    return data

def getDates(data,datatype):
    """

    :param data:
    :return:
    """

    from tqdm import tqdm

    if datatype == 'comments':
        # seperate data
        from PreProcessing import calcDateTime
        print('getting dates of instagram data')
        instagram_comments = data[data['source'] == 'instagram']
        facebook_comments = data[data['source'] == 'facebook']

        # get date of instagram data
        from PreProcessing import calcDateTime
        instagram_comments = calcDateTime(instagram_comments, 'time', True)

        # get date of facebook data
        print('getting dates of facebook data')
        facebook_comments['date'] = facebook_comments['time'].astype(str).apply(lambda date: date[:10])
        facebook_comments['year'] = facebook_comments['time'].astype(str).apply(lambda date: date[6:10])
        # append data
        data = instagram_comments.append(facebook_comments)

    else:
        #posts = event_posts
        data['time'] = data['time'].astype(str)

        german_months = ['januar', 'februar', 'märz', 'april', 'mai', 'juni', 'juli', 'august', 'september', 'oktober','november','dezember', 'mrz','okt', 'dez']
        english_months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'march', 'october', 'december']
        counter = 0
        data.reset_index(drop = True, inplace = True)

        data.time = data.time.str.lower()

        for german, english in zip(german_months, english_months):
            for i in range(data.shape[0]):
                data.time.iloc[i] = data.time.iloc[i].replace(german,english)

        data['date'] = pd.to_datetime(data['time'])

    return data

##########################
# try to handle emoijys###
##########################

def translateEmot(string):

    """

    :param data: dataframe
    :return: data with transalted emojis -> string
    """

    from tqdm import tqdm
    import emot
    import pandas as pd
    import re
    import pickle
    # upload scraped data

    original_string = string
    emot_obj = emot.core.emot()

    try:

        bulk_obj = emot_obj.bulk_emoji(string)
        position = 0
        bulk_obj = emot_obj.bulk_emoji(string)

        for decompose in bulk_obj:

            position = position + 1

            if decompose['mean'] == []:
                translation = original_string
                continue

            else:

                try:

                    meaning = decompose['mean'][0].replace(':','')
                    #print(f'{string[position-1]} is replace by {meaning}')
                    translation = string.replace(string[position-1], meaning)
                except:
                    translation = original_string

    except:

        translation = original_string

    return translation


def removeEmot(string):

    import re

    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags 
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)




#####################
# filter out links###
#####################

def dropLinkageRep(data,text_col):

    org_shape = data.shape[0]
    data = dropLinkage(data,text_col)
    counter = 1
    current_shape = 0
    new_shape = 1

    while current_shape != new_shape:

        current_shape = data.shape[0]
        data = dropLinkage(data, 'text')
        new_shape = data.shape[0]
        counter = counter + 1

    print(f'deleted {org_shape - new_shape} after {counter} iterations')
    return data


def dropLinkage(data,column):

    """

    :param data: dataframe
    :param column: name of the text column
    :return: dataframe with dropped single linkages
    """

    from tqdm import tqdm
    import warnings
    warnings.filterwarnings("ignore")

    data_org = data.copy()
    comments = data[column].fillna('a')


    for comment,comment_count in zip(tqdm(comments), range(data.shape[0])):

        try:

            words = comment.split(' ')

            for word in words:

                if '@' in word and len(words) == 1:

                    data.drop(comment_count, inplace = True)

                elif '@' in word:

                    words.remove(word)
                    data[column][comment_count] = ' '.join(words)

        except:
            pass

    print(f'dropped {data_org.shape[0] - data.shape[0]} rows')

    """
    # remove linked persons
    data = comments[(comments['company'].str.contains('wehkamp')) & (comments['word_count'] > 3)].reset_index(drop = True)
    text = data.text.loc[3]
    import en_core_web_sm
    nlp = en_core_web_sm.load()

    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]


    persons = []
    data['text_cleaned'] = data.text
    for idx,text in tqdm(enumerate(data.text)):
        doc = nlp(text)
        entities = [(ent.text, ent.label_) for ent in doc.ents]

        for entity in range(len(entities)):
                if entities[entity][1] == 'PERSON':


                    for name in entities[entity][0].split():

                        try:
                            data.text_cleaned.loc[idx] = data.text_cleaned.loc[idx].split()
                            data.text_cleaned.loc[idx].remove(name)
                            data.text_cleaned.loc[idx] = ' '.join(data.text_cleaned.loc[idx])
                        except:
                            pass


        for person in persons:
            for idx in range(data.shape[0]):
                if person in data.text.loc[idx]:
                    data.text.at[idx,'text_cleaned'] = data.text.loc[idx].split()
            list(data.text).remove(person)
    """
    return data.reset_index(drop = True)

def getLikes(data,column):

    from tqdm import tqdm

    data[column].fillna('a', inplace = True)

    for i in tqdm(range(data.shape[0])):

        if 'Gefällt' in data[column].iloc[i]:

            data[' likes'].iloc[i] = data[column].iloc[i].split(' ')[2]
            data[column].iloc[i] = data[column].iloc[i].split(' ')[0]

        else:
            data[' likes'].iloc[i] = 0
            data[column].iloc[i] = data[column].iloc[i].split(' ')[0]

    return data

def adjDate(data, frequence):

    from datetime import timedelta
    from tqdm import tqdm

    for date in tqdm(range(data.shape[0])):

        data[' time'][date] = data['timestamp'] - timedelta(weeks = int(data[' time'][date]))

    return data

def googleTranslator(data, column):
    """

    :param document:  word or sentence to be translated
    :return: translated document in english
    """
    import translators as ts
    from tqdm import tqdm
    import pandas as pd
    from datetime import datetime

    #complete_data = pd.read_excel('comments_finaldetected.xlsx')

    #complete_data = complete_data[(complete_data['original_language_short'] == 'en')]

    #complete_data.to_excel('comments_semifinal.xlsx')
    #data = pd.read_excel('data_translatedbatch_8_17.xlsx')
    #done_data = data[data['text_translated'] != 0]
    #done_data['text'] = done_data['text_translated']

    #complete_data.drop(remaining_data.)
    #dat = pd.read_excel('comments_finaldetected.xlsx')
    #remaining_data = data[data['text_translated'] == 0]
    remaining_data = pd.read_excel('data_translatedbatch_remaining_15_10.xlsx')
    data = remaining_data
    data.drop(data[data['company'].str.contains('wehkamp')].index, inplace = True)
    data['text'] = data['text'].astype(str)
    data.drop(data[data['text'] == 'nan'].index, inplace = True)
    data.reset_index(inplace = True, drop = True)

    from PreProcessing import dropLinkageRep
    data = dropLinkageRep(data,'text')
    data.reset_index(inplace = True, drop = True)

    doc_pos = data[data['text_translated']==0].index.min()
    #index = 5000

    import time
    #time.sleep(5)

    save = 0

    while doc_pos != 85766:
        index = doc_pos

        for doc_pos, doc in zip(tqdm(range(index,data.shape[0])),data['text'][index:]):#data_copy.shape[0])):

            try:
                data['text_translated'][doc_pos] = ts.google(doc, to_language= 'en')
                    #print(f'{doc_pos}')
                save = save + 1
            except:
                data['text_translated'][doc_pos] = ts.sogou(doc, to_language= 'en')
                #print(f'{doc_pos}')
                save = save + 1


            if save % 100 == 0:
                    print('storing batch')
                    data.to_excel(f'data_translatedbatch_remaining_{datetime.now().day}_{datetime.now().hour}.xlsx')
            else:
                pass

    #  data_copy.to_excel('ottosburger_translated_finished.xlsx')

    return data

def langDetector(data,text_col):
    """

    :param data:
    :param text_col:
    :return:
    """

    #data = pd.read_excel('data_langdetectbatch.xlsx')
    from langdetect import detect
    from googletrans import Translator
    from tqdm import tqdm
    import pandas as pd

    # initialize new column
    ##data['original_language'] = 0



    # initalize translator
    translator = Translator()
    text_col = 'text'

    data['text'] = data['text'].astype(str)

    for i, str in zip(tqdm(range(710000,data.shape[0])),data[text_col][710000:]):

        try:

            data['original_language_short'].loc[i] = translator.detect(str).lang

            if i % 10000 == 0:

                data.to_excel('data_langdetectbatch.xlsx')

            else:
                pass
        except:
            data['original_language_short'].loc[i] = 'not detected'

    return data

def spellCorrection(data, text_col, lang_col):
    """

    :param data:
    :param text_col:
    :param lang_col:
    :return:
    """
    from tqdm import tqdm
    from spellchecker import SpellChecker
    spell = SpellChecker(language = 'de')
    misspelled = spell.unknown('')

    data['text_corrected'] = 0

    for string_i, string in enumerate(data.text_col):

        data.text_corrected.loc[string_i] = spell.unknown()




# Checks the data for missing values (na) and (optionally) removes them.
#
#   data: data object (pd.DataFrame)
#
#   keepNA: the rows containing nas are not dropped, the function then only returns the na count per column and number
#           or rows containing at least one na
#
#   keepOldIDs: the pd.dropna function drops the rows containing nas, but does not change the indexing, which leads to
#               missing index values. Therefore, for loops iterating over row-indices will not work anymore. Therefore,
#               the index is replaces by a new index. If the old index is import it can be saved as column of
#               the DataFrame

def dataCheck(data: pd.DataFrame, keepNA: bool = False, keepOldIDs: bool = False) -> pd.DataFrame:
    """
    Checks the data for missing values (na) and (optionally) removes them.

    :param data: data object (pd.DataFrame)
    :param keepNA: the rows containing nas are not dropped, the function then only returns the na count per column and number or rows containing at least one na
    :param keepOldIDs: the pd.dropna function drops the rows containing nas, but does not change the indexing, which leads to missing index values. Therefore, for loops iterating over row-indices will not work anymore. Therefore, the index is replaces by a new index. If the old index is import it can be saved as column of the DataFrame
    :return: DataFrame
    """
    print(data.isna().sum())

    rowwiseIsNA = np.vectorize(pd.isna)
    rowwiseNA = rowwiseIsNA(data)
    numRemovedRows = sum([any(row) for row in rowwiseNA])

    if not keepNA:
        data = data.dropna(axis = 0)
        data = data.reset_index(drop=not keepOldIDs)

    return data


# Wrapper function for the text preprocessing. Applies the following text preprocessing steps:
#   1) removes capitalisation (output column of this step: 'proText_C')
#   2) removes punctuation (output column of this step: 'proText_CP')
#   3) removes stopwords (output column of this step: 'proText_CPS')
#   4) Lemmitazation (stemming) of the tokens (output column of this step: 'proText_CPSL')
#
#   data: data object (pd.DataFrame)
#
#   input_col: name of the column containing the original text
#
#   keepSteps: Boolean-Value which determines if the columns which results after each of the four steps are
#              dropped or kept
#
#   keepOriginalData: Boolean-Value which determines if the original text data is saved or not
#
def textPreprocessing(data: pd.DataFrame, input_col: str = 'text', keepSteps: bool = True, keepOriginalData: bool = True) -> pd.DataFrame:
    """
    Wrapper function for the text preprocessing. Applies the following text preprocessing steps:
        1) removes capitalisation (output column of this step: 'proText_C')
        2) removes punctuation (output column of this step: 'proText_CP')
        3) removes stopwords (output column of this step: 'proText_CPS')
        4) Lemmitazation (stemming) of the tokens (output column of this step: 'proText_CPSL')

    :param data: data object (pd.DataFrame)
    :param input_col: name of the column containing the original text
    :param keepSteps: Boolean-Value which determines if the columns which results after each of the four steps are dropped or kept
    :param keepOriginalData: Boolean-Value which determines if the original text data is saved or not
    :return: Data Frame
    """
    data = textCapitalisation(data)
    data = textRmPunctuation(data)
    data = textRmStopwords(data)
    data = textLemmatization(data)

    data[str(str(input_col) + "_original")] = data[input_col]
    data[input_col] = data['proText_CPSL']

    if not keepSteps:
        data = data.drop(['proText_C', 'proText_CP', 'proText_CPS', 'proText_CPSL'], axis = 1)

    if not keepOriginalData:
        data = data.drop([str(str(input_col) + "_original")], axis = 1)

    return data


# Function which covers the removal of the capitalisation. Therefore, the np.vectorize function is used to create a new
# function which returns an array of strings. Each element in this array of strings is created from one element of the
# text column to which the str.lower function is applied. This procedure basically leads to an elementwise application
# of the str.lower function. The results are then saved in a new column.
#
#   data: Data object (pd.DataFrame)
#
#   input_col: Name of the column containing the strings of the previous step.
#
#   output_col: Name of the column which will contain the texts after the application of the current
#               step/transformation.
#
def textCapitalisation(data: pd.DataFrame, input_col: str = 'text', output_col: str ='proText_C') -> pd.DataFrame:
    """
    Function which covers the removal of the capitalisation. Therefore, the np.vectorize function is used to create a new
    function which returns an array of strings. Each element in this array of strings is created from one element of the
    text column to which the str.lower function is applied. This procedure basically leads to an elementwise application
    of the str.lower function. The results are then saved in a new column.

    :param data: Data object (pd.DataFrame)
    :param input_col: Name of the column containing the strings of the previous step.
    :param output_col: Name of the column which will contain the texts after the application of the current step/transformation.
    :return: DataFrame which includes a text colum where the punctuation was removed
    """
    rmCap = np.vectorize(str.lower)
    data[output_col] = rmCap(data[input_col])

    return data


# Function which covers the removal of punctuation. Therefore, the punctuation, digits from the strings library and some
# costume strings are used. With the help of the maketrans and translate function each punctuation and digit character
# is removed (or rather replaced by '') in each text. Additionally, double spaces are replaced, which could result from
# the removal of punctuation (e.x. "We spend 100$ on xy." => "We spend  on xy"). Afterwards leading and trailing
# spaces are removed from the texts.
#
#   data: Data object (pd.DataFrame)
#
#   input_col: Name of the column containing the strings of the previous step.
#
#   output_col: Name of the column which will contain the texts after the application of the current
#               step/transformation.
#
def textRmPunctuation(data: pd.DataFrame, input_col: str = 'proText_C', output_col: str = 'proText_CP') -> pd.DataFrame:
    """
    Function which covers the removal of punctuation. Therefore, the punctuation, digits from the strings library and some
    costume strings are used. With the help of the maketrans and translate function each punctuation and digit character
    is removed (or rather replaced by '') in each text. Additionally, double spaces are replaced, which could result from
    the removal of punctuation (e.x. "We spend 100$ on xy." => "We spend  on xy"). Afterwards leading and trailing
    spaces are removed from the texts.

    :param data: Data object (pd.DataFrame)
    :param input_col: Name of the column containing the strings of the previous step
    :param output_col: Name of the column which will contain the texts after the application of the current step/transformation.
    :return: DataFrame which includes a text colum where the punctuation was removed
    """
    for s in range(data.shape[0]):
        data.loc[s, output_col] = data.loc[s, input_col].translate(str.maketrans({".": ". ", ",": ", ", "?": "? ", "!": "! "}))
        data.loc[s, output_col] = data.loc[s, output_col].translate(
            str.maketrans('', '', string.punctuation + string.digits + "–" + "‘"))  # .replace("'", "")

        data.loc[s, output_col] = str(data.loc[s, output_col]).replace('      ', ' ')
        data.loc[s, output_col] = str(data.loc[s, output_col]).replace('     ', ' ')
        data.loc[s, output_col] = str(data.loc[s, output_col]).replace('    ', ' ')
        data.loc[s, output_col] = str(data.loc[s, output_col]).replace('   ', ' ')
        data.loc[s, output_col] = str(data.loc[s, output_col]).replace('  ', ' ')

    rmLeadSpace = np.vectorize(str.lstrip)
    data[output_col] = rmLeadSpace(data[output_col])

    return data


# Function which covers the removal of stopwords. For that the nltk packages is used which contains a list
# of common (english) stopwords. To this list were some obviously missing words added. Because of the previous step,
# the stopwords have to be stripped of their punctuation. Then each string element is seperated into its tokens and all
# tokens which are in the list of stopwords are removed. Afterwards a new string is created. The results have shown
# that some special characters where not removed correctly, therefore the punctuation is removed again.
# After that the resulting word list and word frequency's in the corpus where analysed (upper 5% of word frequencies)
# and a list of additional stopwords (with high frequencies) was created, which are then removed separately.
#
#   data: Data object (pd.DataFrame)
#
#   input_col: Name of the column containing the strings of the previous step.
#
#   output_col: Name of the column which will contain the texts after the application of the current
#               step/transformation.
#
def textRmStopwords(data: pd.DataFrame, input_col ='proText_CP', output_col ='proText_CPS') -> pd.DataFrame:
    """
    Function which covers the removal of stopwords. For that the nltk packages is used which contains a list
    of common (english) stopwords. To this list were some obviously missing words added. Because of the previous step,
    the stopwords have to be stripped of their punctuation. Then each string element is seperated into its tokens and all
    tokens which are in the list of stopwords are removed. Afterwards a new string is created. The results have shown
    that some special characters where not removed correctly, therefore the punctuation is removed again.
    After that the resulting word list and word frequency's in the corpus where analysed (upper 5% of word frequencies)
    and a list of additional stopwords (with high frequencies) was created, which are then removed separately.

    :param data: Data object (pd.DataFrame)
    :param input_col: Name of the column containing the strings of the previous step.
    :param output_col: Name of the column which will contain the texts after the application of the current step/transformation.
    :return: DataFrame which includes a text colum where the stopwords were removed
     """
    nltk.download('stopwords')
    nltk.download('punkt')

    all_stopwords = stopwords.words('english')
    all_stopwords.extend(["i'm", "it'll"])

    all_stopwords_noPunc = []
    for word in all_stopwords:
        all_stopwords_noPunc.append(word.translate(str.maketrans('', '', string.punctuation + string.digits + "–")))

    all_stopwords.extend(all_stopwords_noPunc)

    tk = SpaceTokenizer()

    for s in range(data.shape[0]):
        tokens = tk.tokenize(data.loc[s, input_col])
        tokens_noStop = [word for word in tokens if not word in all_stopwords]
        data.loc[s, output_col] = (" ").join(tokens_noStop)
        data.loc[s, output_col] = data.loc[s, output_col].translate(
            str.maketrans('', '', string.punctuation + string.digits + "’"))
        data.loc[s, output_col] = str(data.loc[s, output_col]).replace('  ', ' ')

    additional_stopwords = ["were", "us", "going", "thats", "well", "weve", "one", "also", "q", "–", "still", "its",
                            "g", "theyre", "may", "ill", "id", "dont", "ive", "cant", "theyve", "im", "youre", "hi"]

    for s in range(data.shape[0]):
        tokens = tk.tokenize(data.loc[s, output_col])
        tokens_noStop = [word for word in tokens if not word in additional_stopwords]
        data.loc[s, output_col] = (" ").join(tokens_noStop)

    return data

# Function which covers lemmatization. Again the nltk packages is used here. Each element in the text columns is again
# seperated into its tokens and then the stem of the corresponding token is returned and combined back to a string.
#
#   data: Data object (pd.DataFrame)
#
#   input_col: Name of the column containing the strings of the previous step.
#
#   output_col: Name of the column which will contain the texts after the application of the current
#               step/transformation.
#
def textLemmatization(data: pd.DataFrame, input_col: str ='proText_CPS', output_col: str ='proText_CPSL') -> pd.DataFrame:
    """
    Function which covers lemmatization. Again the nltk packages is used here. Each element in the text columns is again
    seperated into its tokens and then the stem of the corresponding token is returned and combined back to a string.

    :param data: Data object (pd.DataFrame)
    :param input_col: Name of the column containing the strings of the previous step.
    :param output_col: Name of the column which will contain the texts after the application of the current step/transformation.
    :return: DataFrame which includes a text colum where the stemming was done
    """
    nltk.download('wordnet')
    nltk.download('omw-1.4')

    tk = SpaceTokenizer()
    lemmatizer = WordNetLemmatizer()

    for s in range(data.shape[0]):
        tokens = tk.tokenize(data.loc[s, input_col])
        data.loc[s, output_col] = (" ").join([lemmatizer.lemmatize(w) for w in tokens])

    return data


def dataSelection(data: pd.DataFrame, label_stage: int, doc_type: str = 'question'):
    """
    Function which handles the filtering of the data.

    :param data: DataFrame of the data
    :param label_stage: Integer value of the label stage (range from 1 to 3)
    :param doc_type: Type of the document (range 'question', 'answer')
    :return: returns a DataFrame with the filtered data
    """
    label_stage_str = f"label_l{label_stage}"
    type = {'question': 'QID', 'answer': 'AID'}
    type_label = {'question': 'Question', 'answer': 'Answer'}
    label_str = f"{type_label[doc_type]}_{label_stage}"

    selection_id = data['label_id'].str.contains(type[doc_type])
    selection_label = data[label_stage_str].str.contains(label_str)

    return data.loc[[x&y for x, y in zip(selection_id, selection_label)], :]


def dataSample(documentRep: pd.DataFrame, data: pd.DataFrame, method: str, n: int, col: str = 'label_l1', folds = 10) -> pd.DataFrame:
    """
    Handles the data sampling.

    :param documentRep: Document representation
    :param data: dataset
    :param method: method of sampling ('undersample', 'resample')
    :param n: desired length of sampled data (obsolete for undersampling because the number of samples of the smallest group is set as n)
    :param col: name of the target column
    :param folds: Number of folds the data should be devided in. The fold number is determined by "sample draw without replacement" and is added as integer column to the resulting dataframe
    :return: DataFrame of document representation in 'doc'-column and labels in 'labels'-column and the fold number in the 'fold'-column
    """
    labels = np.unique(data[col])
    label_counts = {}
    for l in labels:
        label_counts[l] = label_counts[l] = data[col].str.cat(sep = " ").split().count(str(l))

    data_groups = {}
    for l in labels:
        data_groups[l] = data.loc[data[col] == l, :].index

    vectors = []
    categories = []
    if method == "undersample":
        n = min(label_counts.values())
        for l in labels:
            # selection_index.extend(np.random.choice(data_groups[l], n))
            vectors.extend(documentRep.loc[np.random.choice(data_groups[l], n, replace=True), :])
            categories.extend(data.loc[np.random.choice(data_groups[l], n, replace=True), col])

    elif method == "resample":
        for l in labels:
            #selection_index.extend(np.random.choice(data_groups[l], n, replace = True))
            vectors.extend(documentRep.loc[np.random.choice(data_groups[l], n, replace = True), :] )
            categories.extend(data.loc[np.random.choice(data_groups[l], n, replace = True), col] )

    elif method == 'random':
        oversampler = RandomOverSampler(random_state = 42)
        vectors, categories = oversampler.fit_resample(documentRep, data[col])

    elif method == 'SMOTE':
        oversampler = SMOTE(random_state = 42)
        vectors, categories = oversampler.fit_resample(documentRep, data[col])

    elif method == 'BorderlineSMOTE':
        oversampler = BorderlineSMOTE(random_state = 42)
        vectors, categories = oversampler.fit_resample(documentRep, data[col])

    data_sample = pd.DataFrame()
    #vectors = pd.DataFrame(vectors)
    #categories = pd.DataFrame(categories)
    #return vectors, categories
    n = len(categories)
    fold_n = int(np.ceil(n / folds))
    fold_vec = np.array([i+1 for i in range(folds)] * fold_n)
    data_sample['doc'] = vectors.to_numpy().tolist()
    data_sample['labels'] = categories
    data_sample['fold'] = np.random.choice(fold_vec, n, replace = False)

    return data_sample


def dataSplit(data: pd.DataFrame, test_fold = 10):
    """
    Handles the splitting of data (data and document representation that is, every data as long the row-index matches the doc-index)

    :param data: DataFrame of data
    :param test_fold: number of the fold, that should be used for the test data
    :return: tuple of train and test data
    """
    data = data.sample(frac=1)

    #split_index = np.quantile(data.index, train_split_frac) # np.floor(train_split_frac * data.shape[0])
    dataTrain, dataTest = data.loc[data['fold'] != test_fold, :], data.loc[data['fold'] == test_fold, :]
    return dataTrain, dataTest


def generateEncodingMatrix(data: pd.Series) -> pd.DataFrame:
    """
    Generates a DataFrame which holds a 'index' column (number) and a 'value' column (labels). Each column can be set as index and therefore a translation between labels and number representation is possible.

    :param data: Series of labels
    :return: DataFrame
    """
    labels = np.unique(data)
    encoding_matrix = pd.DataFrame([[i, v] for i, v in enumerate(labels)], columns=['index', 'value'])
    return encoding_matrix


def dataEncoding(data: pd.Series, encoding_matrix: pd.DataFrame):
    """
    Handles the encoding (translation from text labels to number representation) of the labels.

    :param data: Series of (text) labels
    :param encoding_matrix: encoding_matrix (see generateEncodingMatrix)
    :return: array of encoded data
    """
    return encoding_matrix.set_index('value').loc[data, 'index'].values


def dataDecoder(data, encoding_matrix):
    """
    Handles the decoding (translation from number representation labels to text labels) of the labels.

    :param data: array/Series of numbers
    :param encoding_matrix: encoding_matrix (see generateEncodingMatrix)
    :return: array of decoded data
    """
    return encoding_matrix.set_index('index').loc[data, 'value'].values

def oversampling(word_emebedding, labels, method):

    '''
    :param word_emebedding: word_embedding matrix
    :param labels: labels from data
    :param method: oversampling method
    :return:
    '''

    if method == 'random':
        oversampler = RandomOverSampler(random_state = 42)
        vectors, categories = oversampler.fit_resample(word_emebedding, labels)

    if method == 'SMOTE':
        oversampler = SMOTE(random_state = 42)
        vectors, categories = oversampler.fit_resample(word_emebedding, labels)

    if method == 'BorderlineSMOTE':
        oversampler = BorderlineSMOTE(random_state = 42)
        vectors, categories = oversampler.fit_resample(word_emebedding, labels)

    return vectors, categories


def companySelector(data, company_list):

    selected_data = data[data['company'] == company_list[0]]

    for company in company_list[1:]:

        sub_data = data[data['company'] == company]
        selected_data = selected_data.append(sub_data)

    return selected_data


def textPreProcessing2(data, column):

    import nltk
    nltk.download('stopwords')
    nltk.download('wordnet')
    from nltk.corpus import stopwords
    from nltk import word_tokenize, sent_tokenize
    ps = nltk.porter.PorterStemmer()
    import unicodedata
    import re
    import numpy as np

    def basic_clean(original):
        word = original.lower()
        word = unicodedata.normalize('NFKD', word) \
            .encode('ascii', 'ignore') \
            .decode('utf-8', 'ignore')
        word = re.sub(r"[^a-z0-9'\s]", '', word)
        word = word.replace('\n',' ')
        word = word.replace('\t',' ')
        return word

    def remove_stopwords(original, extra_words=[], exclude_words=[]):
        stopword_list = stopwords.words('english')

        for word in extra_words:
            stopword_list.append(word)
        for word in exclude_words:
            stopword_list.remove(word)

        words = original.split()
        filtered_words = [w for w in words if w not in stopword_list]

        original_nostop = ' '.join(filtered_words)

        return original_nostop

    def stem(original):
        ps = nltk.porter.PorterStemmer()
        stems = [ps.stem(word) for word in original.split()]
        original_stemmed = ' '.join(stems)
        return original_stemmed

    df = data
    docs = []
    for sentence in df.text:
        words = word_tokenize(stem(remove_stopwords(basic_clean(sentence))))
        docs.append(words)

    return docs

def getCompanyRanges(data, company):

    """
    :param data:
    :param company:
    :return:
    """

    company_dict = data[data['company'] == company].index.values
    return company_dict

def embeddingCompanyMatch(data, document_vectors):

    # 1. get company ranges
    from PreProcessing import getCompanyRanges
    company_ranges = getCompanyRanges(data, data.company.unique())

    # 2. create company vector
    company_vector = []

    # 3. fill company vector according to calculated ranges from step 1
    for ranges, company in zip(company_ranges.values(), company_ranges.keys()):

        company_vector.extend([company] * ranges)


    # 4. Stack company vector to document vectors
    document_vectors = np.c_[document_vectors, company_vector]

    return document_vectors, company_ranges

#%%
